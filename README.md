
# Abstract Topic Distribution & Classification App

This Streamlit application allows you to:

1. **Preprocess** a user‑provided abstract and display its word frequency.
2. **Compute topic distribution** for the abstract using a pre‑trained (or auto‑trained) BERTopic model.
3. **Classify** the abstract’s journal using two feature sets:

   * **Topic‑only** features (topic probabilities from BERTopic)
   * **Topic+Term** features (concatenation of topic probabilities and term‑frequency counts)

   Three classifiers are provided for each feature set:

   * Logistic Regression
   * Random Forest
   * Support Vector Machine (SVC)

The application will automatically train and save the BERTopic model and classifiers on first run (if they do not already exist). Subsequent launches will load the serialized objects, making startup faster.

---

## Folder Structure

```text
/your_app_folder
│
├── app.py
├── WOS_data.xlsx
├── bertopic_bundle.pkl           ← autogenerated on first run
├── classification_bundle.pkl     ← autogenerated on first run
├── run_app.bat                   ← optional batch script to launch the app
├── requirements.txt              ← list of Python dependencies
└── README.md                     ← this file
```

* **app.py**
  The main Streamlit script. Contains all logic for:

  * Loading or training the BERTopic model
  * Loading or training the classification models
  * Preprocessing user input
  * Displaying word‑frequency, topic distribution, and classification results

* **WOS\_data.xlsx**
  Raw Excel data (must be present before first run). Expected columns:

  * **Abstract**: Text of each paper’s abstract
  * **Authors**: Authors (used only in preprocessing pipeline)
  * **Journal**: Target label for classification
  * Additional metadata columns are ignored by the app.

* **bertopic\_bundle.pkl**
  Binary pickle that stores:

  * The trained `BERTopic` instance
  * A DataFrame of topic probabilities for every document in `WOS_data.xlsx`
  * A NumPy array of shape `(n_docs, n_topics)` representing topic distributions
  * The fitted `CountVectorizer` (term–document matrix)
  * A DataFrame of term counts for each document
  * List of topic column names (`["Topic 0", "Topic 1", …, "Topic 49"]`)
  * The list of raw abstracts and the Series of Journal labels

  **Behavior:**

  * If `bertopic_bundle.pkl` does not exist, `app.py` will load `WOS_data.xlsx`, preprocess all abstracts, train BERTopic (with a fixed random seed for reproducibility), build a 2000‑term CountVectorizer, and save everything as this pickle.
  * If `bertopic_bundle.pkl` exists, it will be loaded directly without retraining.

* **classification\_bundle.pkl**
  Binary pickle that stores:

  * Three trained classifiers for the “topic” feature set:

    * Logistic Regression (with its `StandardScaler`)
    * Random Forest (with its `StandardScaler`)
    * SVC (with its `StandardScaler`)
  * Three trained classifiers for the “topic\_term” feature set (same algorithms, same scalers)

  **Behavior:**

  * If `classification_bundle.pkl` does not exist, `app.py` will load (or train) the BERTopic bundle, build feature matrices (topic‑only and topic+term) for all documents, train each of the six model+scaler pairs, and save them here.
  * If it already exists, models and scalers are loaded directly.

* **run\_app.bat** (Windows‑only, optional)
  Example batch script to launch `streamlit run app.py` from this directory. If you’re on macOS/Linux, you can simply run:

  ```
  streamlit run app.py
  ```

* **requirements.txt**
  Lists all Python packages required to run the app. Example contents:

  ```
  streamlit
  pandas
  numpy
  nltk
  bertopic
  umap-learn
  hdbscan
  sentence-transformers
  scikit-learn
  ```

---

## Prerequisites

1. **Python 3.8+**
2. **Git** (optional, if you want to clone this repo)
3. A working internet connection on first run to download:

   * NLTK data (stopwords, punkt tokenizer, WordNet)
   * SentenceTransformers model (`all-MiniLM-L6-v2`)
   * Any underlying dependencies for BERTopic, UMAP, HDBSCAN

---

## Installation & Setup

```bash
# Clone / download the folder, then
cd /path/to/your_app_folder

# (Recommended) create & activate a virtual environment
python -m venv .venv

# Windows
.venv\Scripts\activate
# macOS/Linux
source .venv/bin/activate

# Install dependencies
pip install --upgrade pip
pip install -r requirements.txt
```

Make sure **`WOS_data.xlsx`** is present before the first run.

---

## Running the App

```bash
streamlit run app.py
```

* **First launch** will train BERTopic and the six classifiers, then save two pickles.
* **Subsequent launches** load both pickles — startup is much faster.

---

## How It Works

1. **Data Loading** from `WOS_data.xlsx`.
2. **Text Preprocessing** (lowercase, tokenize, stopword removal, lemmatize, DF filtering).
3. **Topic Modeling** via BERTopic (SentenceTransformers → UMAP → HDBSCAN).
4. **Term‑Document Matrix** via `CountVectorizer(max_features=2000)`.
5. **Classification** with Logistic Regression, Random Forest, and SVC on two feature sets.
6. **User Abstract** analysis: word frequencies, topic distribution, and six journal predictions.

---

## Troubleshooting

* **Missing NLTK data** → ensure internet access on first run.
* **`WOS_data.xlsx` not found** → verify it’s in the same folder as `app.py`.
* **Streamlit port conflict** → run `streamlit run app.py --server.port 8502`.

---

## License

This project is released under an Apache-2 license

## Contact & Contribution
If you have any questions, issues, or suggestions, please open an issue on the repository or email y.sun@eng.famu.fsu.edu (Dr. Yanshuo Sun), m.zhao@lehigh.edu (Dr. Meng Zhao), or sc20hw@fsu.edu (Dr. Shijie Chen).
